[
  {
    "path": "posts/2021-12-08-affine-transformation-of-sf-objects/",
    "title": "Affine transformations of sf objects",
    "description": "Manipulating simple features in `sf` is sorta simple, sorta not...",
    "author": [],
    "date": "2021-12-16",
    "categories": [],
    "contents": "\n\n\nknitr::opts_chunk$set(error = TRUE, message = TRUE)\n\n\n\nPackages\n\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(dplyr)\nlibrary(wk)\n\n\n\nA simple square\nJust to get things set up let’s make a simple square.\n\n\nsquare <- (st_polygon(list(matrix(c(-1, -1, 1, -1, 1, 1, -1, 1, -1, -1), \n                                 5, 2, byrow = TRUE))) * 0.5 + c(1, 0)) %>%\n  st_sfc()\n\ntm_shape(square) + \n  tm_borders(col = \"red\") + \n  tm_grid()\n\n\n\n\nSimple transformations\nIn the code above, we made a polygon and multipled it by 0.5, then added c(1,0) to it. This had the effect of scaling it by 0.5 andthen translating it by the vector \\[\\left[\\begin{array}{c}1\\\\0\\end{array}\\right]\\]\nThese unlikely looking operations are perfectly valid, although they feel a bit ‘off’.\nEven more unlikely is that you can multiply an sf object by a matrix…\n\n\nang <- pi / 6\nmat <- matrix(c(cos(ang), -sin(ang), \n                sin(ang),  cos(ang)), 2, 2, byrow = TRUE)\n(square * mat) %>% \n  plot()\n\n\n\n\nThis is very… but probably also a bad idea! Because you have to post-multiply by the matrix, the sense of many affine transformations is reversed and construction of the matrix is not ‘by the book’. Usually the affine transformation matrix \\(\\mathbf{A}\\) for an anti-clockwise rotation by angle \\(\\theta\\) around the origin, would be\n\\[\n\\mathbf{A} = \n\\left[\\begin{array}{cc}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{array}\\right]\n\\]\nHere, because we are post-multiplying the rotation will be in the other direction… and to rotate anti-clockwise, you use the \\(-\\mathbf{A}=\\mathbf{A}^T\\)\n\\[\n-\\mathbf{A} = \n\\left[\\begin{array}{cc}\n-\\cos\\theta & \\sin\\theta \\\\\n-\\sin\\theta & -\\cos\\theta\n\\end{array}\\right] = \n\\left[\\begin{array}{cc}\n\\cos\\theta & \\sin\\theta \\\\\n-\\sin\\theta & \\cos\\theta\n\\end{array}\\right] = \\mathbf{A}^\\mathrm{T}\n\\]\nThis means that if you are doing any serious affine transforming of sf shapes at a low-level in R spatial, I recommend either writing some wrapper functions that generate and apply the necessary matrices on the fly, or, probably better yet, using the wk package which has proper support for affine transformations.\nWrapper functions for the ‘native’ matrix operations\nTaking the wk approach, I will show what you can do below. Making similar functions that just post-multiply shapes or add vectors to them instead is left as an exercise for the reader…\nFor example a rotation function might look something like\n\n\n# for convenience angle in degrees\nrotate_sf <- function(shp, angle) {\n  a <- angle * pi / 180\n  wk::wk_transform(shp, wk::wk_affine_rotate(angle))\n}\n\n\n\nand this can be applied like this\n\n\nbase_s <- st_polygon(list(matrix(c(.25, 0.25, \n                                   1.5, 0.25, \n                                   1.5, 1.5, \n                                   .25, 1.5, \n                                   .25, .25), \n                                 nrow = 5, ncol = 2, byrow = TRUE)))\nplot(base_s, xlim = c(-2, 2), ylim = c(-2, 2), \n     col = \"lightgrey\", border = NA)\nfor (a in seq(0, 330, 30)) {\n  plot(rotate_sf(base_s, a), add = TRUE)\n}\n\n\n\n\nOr you might want to make multiple copies of a basic unit at a series of locations on a grid. First, make a function that will translate a shape by a vector.\n\n\ntranslate_shape <- function(shape, translation) {\n  wk::wk_transform(shape, wk::wk_affine_translate(translation[1], translation[2]))\n}\n\n\n\nGenerate a set of translations\n\n\ngrid <- expand.grid(x = 0:19 * 1.2 + 1, y = 0:19 * 1.2 + 1)\nsquares <- list()\nfor (i in seq(nrow(grid))) {\n  squares <- append(squares, \n                    list(translate_shape(square, c(grid$x[i], grid$y[i]))))\n}\nsquares %>% sapply(\"[\") %>% st_sfc() %>%\n  plot()\n\n\n\n\nThe wk functions also allow you to compose complex transformations from several steps. For example a function to rotate a shape around its own centre, i.e., not arounf the origin at \\((0,0)\\) requires moving the shape so that its centroid is at the origin, performing the rotation, then moving it back:\n\n\nrotate_around_centroid <- function(shape, angle) {\n  centroid <- st_centroid(shape) %>%\n    st_coordinates() %>%\n    c()\n  transformation <- wk::wk_affine_compose(\n    wk::wk_affine_translate(-centroid[1], -centroid[2]),\n    wk::wk_affine_rotate(angle),\n    wk::wk_affine_translate(centroid[1], centroid[2])\n  )\n  wk::wk_transform(shape, transformation)\n}\n\n\n\nAnd here’s that in action\n\n\nplot(square, xlim = c(-1, 1), ylim = c(-1, 1))\nfor (a in seq(30, 330, 30)) {\n  plot(rotate_around_centroid(square, a), add = TRUE)\n}\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-12-08-affine-transformation-of-sf-objects/distill-preview.png",
    "last_modified": "2021-12-16T16:27:41+13:00",
    "input_file": "affine-transformations-of-sf-objects.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-12-08-low-level-handling-sf-objects/",
    "title": "Low level handling of sf objects",
    "description": "You can handle sf objects at a low level but it can \ntake a bit of getting used to",
    "author": [],
    "date": "2021-12-08",
    "categories": [],
    "contents": "\n\n\nknitr::opts_chunk$set(error = TRUE, message = TRUE)\n\n\n\nPackages\nEverything here needs just sf and dplyr.\n\n\nlibrary(sf)\nlibrary(dplyr)\n\n\n\nMaking polygons\nMy main confusion dealing with polygons in sf sounds dumb, but was easily fixed. Matrices in R get populated by column, by default, where the points in a polygon are in the rows of the matrix (as they would be in a dataframe with x and y attributes). You just have to make sure to populate the matrices in the right order.\nThere’s also the slightly strange fact that you have to wrap a matrix of points in a list to make a polygon.\nSo because of the row-column thing, there’s a tendency to do\n\n\nmat <- matrix(c(0, 0,\n                1, 0,\n                1, 1,\n                0, 1,\n                0, 0), nrow = 5, ncol = 2)\nsquare <- st_polygon(list(mat))\n\n\nError in MtrxSet(x, dim, type = \"POLYGON\", needClosed = TRUE): polygons not (all) closed\n\nBut that fails, because the matrix we made was\n\n\nmat\n\n\n     [,1] [,2]\n[1,]    0    1\n[2,]    0    0\n[3,]    1    1\n[4,]    0    0\n[5,]    1    0\n\nand the first and last rows don’t match (even if they did, it’s not actually a polygon!).\nBut specify that the matrix should be populated byrow and all is well\n\n\nsquare <- st_polygon(list(matrix(c(0, 0,\n                                   1, 0,\n                                   1, 1,\n                                   0, 1,\n                                   0, 0), nrow = 5, ncol = 2, byrow = TRUE)))\nplot(square)\n\n\n\n\nIf you happen to have vectors of the x and y coordinates, then it’s easier.\n\n\nx <- c(0, 1, 1, 0, 0)\ny <- c(0, 0, 1, 1, 0)\nsquare <- st_polygon(list(matrix(c(x, y), nrow = 5, ncol = 2)))\nplot(square)\n\n\n\n\nFloating point coordinates and their discontents\nsf defaults to using floating point calculations which has some annoying side-effects. For example, the code below results in an error\n\n\nangles <- 0:3 * 2 * pi / 3\nx <- cos(angles)\ny <- sin(angles)\ntriangle <- st_polygon(list(matrix(c(x, y), nrow = 7, ncol = 2)))\n\n\nError in MtrxSet(x, dim, type = \"POLYGON\", needClosed = TRUE): polygons not (all) closed\n\nBecause sf defaults to floating point it doesn’t consider the polygon closed due to precision issues that mean R considers sin(0) != sin(2 * pi):\n\n\nsin(0) == sin(2 * pi)\n\n\n[1] FALSE\n\nThere is no easy way to fix this except to round the coordinates!\n\n\nx <- round(x, 6)\ny <- round(y, 6)\ntriangle <- st_polygon(list(matrix(c(x, y), nrow = 4, ncol = 2)))\nplot(triangle)\n\n\n\n\nThere’s not a lot you can do about this when you are constructing sf objects. Polygons must be closed, and equality is strictly applied to the opening and closing points. You can’t ask st_polygon to automatically close polygons for you.\nOnce you have polygons to work with, the problem can come back to bite you, but there is a way around it. For example, this works OK:\n\n\nsquare %>% \n  st_difference(triangle) %>%\n  plot()\n\n\n\n\nBut let’s make two squares that are theoretically adjacent to one another, but happen to have non-integer coordinates (which… is pretty commonplace!)\n\n\nangles <- seq(1, 7, 2) * 2 * pi / 8\nangles <- c(angles, angles[1])\nx1 <- cos(angles)\ny1 <- sin(angles)\n\ns1 <- st_polygon(list(matrix(c(x1, y1), nrow = 5, ncol = 2)))\nbb <- st_bbox(s1)\ns2 <- s1 + c(bb$xmax - bb$xmin, 0)\n\nplot(s1, xlim = c(-1, 2.1))\nplot(s2, add = TRUE)\n\n\n\n\nTwo squares, next to one another as we might hope, but if, for example, we st_union them we get a MULTIPOLYGON.\n\n\ns3 <- st_union(s1, s2)\ns3\n\n\nMULTIPOLYGON (((0.7071068 0.7071068, -0.7071068 0.7071068, -0.7071068 -0.7071068, 0.7071068 -0.7071068, 0.7071068 0.7071068)), ((2.12132 0.7071068, 0.7071068 0.7071068, 0.7071068 -0.7071068, 2.12132 -0.7071068, 2.12132 0.7071068)))\n\nIf we plot them, they still appear separate\n\n\nplot(s3)\n\n\n\n\nand if we measure the distance between them, turns out they don’t touch at all, but are in fact a miniscule distance apart…\n\n\ns1 %>% st_distance(s2)\n\n\n             [,1]\n[1,] 3.330669e-16\n\nIt’s probably not necessary to point out how silly this is, even if it is strictly correct.\n\n\nRXKCD::getXKCD(2170)$img\n\n\n\n[1] \"https://imgs.xkcd.com/comics/coordinate_precision.png\"\n\nSpecifying precision for spatial operations\nBy contrast if we use rgeos functions the equivalent union operation works as we might expect (although we do have to feed rgeos the old sp types of polygon, which we can do via a call to as(\"Spatial\")…)\n\n\nrgeos::gUnion(as(s1, \"Spatial\"), as(s2, \"Spatial\")) %>%\n  st_as_sfc() %>%\n  plot()\n\n\n\n\nsf does allow us to effectively emulate the rgeos behaviour, albeit not for simple geometries. When we instead bundle geometries up into feature collections, we can assign them a precision, and this will take care of the kinds of problems we see above:\n\n\ns1_sfc <- s1 %>% \n  st_sfc() %>%\n  st_set_precision(1e8)\ns2_sfc <- s2 %>% \n  st_sfc() %>%\n  st_set_precision(1e8)\n\ns1_sfc %>% \n  st_union(s2_sfc) %>%\n  plot()\n\n\n\n\nThe first time I looked this up in help, I got it wrong due to careless reading, and, I think, assuming that the number you provide to st_set_precision() was a ‘tolerance’, or, in effect a ‘snap distance’. The help is also a bit roundabout, and directs you to this page, for an explanation of how it works.\nIn effect all coordinates are adjusted by applying a function like this one:\n\n\nadjust_precision <- function(x, precision) {\n  round(x * precision) / precision\n}\nsqrt(2) %>% adjust_precision(1000)\n\n\n[1] 1.414\n\nst_snap\nAnother possible fix for the floating point issue is snapping points to the coordinates of another object before applying operations. So this works, although it is not as clean as the st_precision option. On the other hand, it does work on plain geometry objects, not only on those that have been bundled up into collections.\n\n\ns1 %>% st_snap(s2, 1e-8) %>%\n  st_union(s2) %>%\n  plot()\n\n\n\n\nIn conclusion\nThe tools for making and manipulating geometries at a low level are available in sf but they are not always as simple as you’d like. Of course, most often you are dealing with datasets and that’s where sf comes into its own. Just remember st_set_precision() and you should be able to avoid quite a few headaches…\n\n\n\n",
    "preview": "posts/2021-12-08-low-level-handling-sf-objects/distill-preview.png",
    "last_modified": "2021-12-16T16:25:54+13:00",
    "input_file": "low-level-sf-objects.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-22-spatstat-idw/",
    "title": "Inverse distance weighted (IDW) interpolation using spatstat",
    "description": "One of the joys (ahem) of R spatial is moving data around between formats so\nyou can use the best packages for particular jobs. Here's an example using IDW\ninterpolation in spatstat",
    "author": [],
    "date": "2021-10-22",
    "categories": [],
    "contents": "\nLibraries\nLibraries are the usual suspects plus spatstat (duh) and maptools for some extra conversions. We also need terra for the data prep.\n\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(maptools)\nlibrary(terra)\n\n\n\nIntroduction\nAs is often the case there is useful functionality in a package that doesn’t play nice with the core R-spatial packages. spatstat is really great for lots of things, but does not support sf and even needs a bit of persuading to handle sp data. Its implementation of IDW interpolation is nice however, so it’s nice to know how to use it. Whether or not you should ever use IDW is another question altogether, but we can worry about that some other time.\nData\nFirst we need a set of values to interpolate. I made a projected version of the R core dataset volcano which is a nice place to start.\n\n\nmaungawhau <-  rast(\"maungawhau.tif\")\n\n\n\nSome random control points and a study area\nWe can get a dataframe of random points on the surface using terra::spatSample. We’ll make this into a sf object as a starting point because that’s the most likely situation when you want to interpolate data (you will have an sf source).\n\n\npts <- maungawhau %>%\n  spatSample(500, xy = TRUE) %>%\n  st_as_sf(coords = c(\"x\", \"y\")) %>%\n  st_set_crs(2193) %>%\n  st_jitter(5)\n\n\n\nWe also need a spatial extent for the interpolation, so let’s just make a convex hull of the points\n\n\nspatial_extent <- pts %>%\n  st_union() %>%\n  st_convex_hull() %>%\n  st_sf()\n\n\n\nAnd just to see where we are at\n\n\ntm_shape(maungawhau) + \n  tm_raster() +\n  tm_shape(pts) + \n  tm_dots() + \n  tm_shape(spatial_extent) + \n  tm_borders() + \n  tm_layout(legend.outside = TRUE)\n\n\n\n\nMake the data into a spatstat point pattern\nspatstat has its own format for point patterns, including coordinates, marks (the values) and a window or owin (the spatial extent). It’s best to make the window first and then we can make the whole thing all at once. spatstat prefers sp objects, so we go via ‘Spatial’ to get a spatstat::owin object. maptools provides the conversion to an owin.\n\n\nW <- spatial_extent %>%\n  as(\"Spatial\") %>%\n  as(\"owin\")\n\n\n\nWe also need the control point coordinates\n\n\nxy <- pts %>%\n  st_coordinates() %>%\n  as_tibble()\n\n\n\nNow we can make a spatstat::ppp point pattern\n\n\npp <- ppp(x = xy$X, xy$Y, marks = pts$maungawhau, window = W)\nplot(pp)\n\n\n\n\nSuccess!\nA previous notebook showed an even quicker way to do this, but where the window will be formed from a bounding box (and where’s the fun in that?)\npts %>%\n  as(\"Spatial\") %>%\n  as.ppp()\nInterpolation\nIt’s easy from here. power is the inverse power applied to distances, and eps is the resolution in units of the coordinate system.\n\n\nresult <- idw(pp, power = 2, eps = 10)\nplot(result)\n\n\n\n\nThis is readily converted back to a terra raster for comparison with the original surface.\n\n\ninterpolation <- rast(result)\ncrs(interpolation) <- crs(maungawhau) # spatstat does not retain CRS information\n\nm1 <- tm_shape(maungawhau) + \n  tm_raster()\nm2 <- tm_shape(interpolation) +\n  tm_raster()\n\ntmap_arrange(m1, m2)\n\n\n\n\nLike I said, IDW is not necessarily a great interpolation method!\n\n\n\n",
    "preview": "posts/2021-10-22-spatstat-idw/distill-preview.png",
    "last_modified": "2021-10-22T01:13:59+13:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-21-kde/",
    "title": "Kernel density estimation in R spatial",
    "description": "Here's one way to do kernel density estimation in R spatial",
    "author": [],
    "date": "2021-10-21",
    "categories": [],
    "contents": "\nPackages\nThis requires a surprising number of moving parts (at least the way I did it):\n\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(spatstat)\nlibrary(maptools)\nlibrary(raster)\nlibrary(RColorBrewer) # because default raster colours are horrible\nlibrary(dplyr)\n\n\n\nData\nThe data are some point data (Airbnb listings from here) and some polygon data (NZ census Statistical Area 2 data).\nLoad the data\n\n\npolys <- st_read(\"sa2.gpkg\")\n\n\nReading layer `sa2' from data source \n  `/home/osullid3/Documents/code/newwebsite/testing/_posts/2021-10-21-kde/sa2.gpkg' \n  using driver `GPKG'\nSimple feature collection with 78 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1735096 ymin: 5419590 xmax: 1759041 ymax: 5443768\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\npts <- st_read(\"abb.gpkg\")\n\n\nReading layer `abb' from data source \n  `/home/osullid3/Documents/code/newwebsite/testing/_posts/2021-10-21-kde/abb.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1254 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1742685 ymin: 5420357 xmax: 1755385 ymax: 5442630\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\nAnd have a look\n\n\ntm_shape(polys) +\n  tm_polygons() + \n  tm_shape(pts) + \n  tm_dots()\n\n\n\n\nspatstat for density estimation\nThe best way I know to do density estimation in the R ecosystem is using the spatstat library’s specialisation of base R’s density function. That means converting the point data to a spatstat planar point pattern (ppp) object, which involves a couple of steps.\n\n\npts.ppp <- pts$geom %>% \n  as(\"Spatial\") %>% # we need to convert to sp as a bridge to spatstat\n  as.ppp()          # and this is what we need maptools for...\n\n\n\nA point pattern also needs a ‘window’, which we’ll make from the polygons.\n\n\npts.ppp$window <- polys %>%\n  st_union() %>%       # combine all the polygons into a single shape\n  as(\"Spatial\") %>%    # convert to sp\n  as.owin()            # convert to spatstat owin - again maptools...\n\n\n\nNow the kernel density\nWe need some bounding box info to manage the density estimation resolution\n\n\nbb <- st_bbox(polys)\ncellsize <- 100\nheight <- (bb$ymax - bb$ymin) / cellsize\nwidth <- (bb$xmax - bb$xmin) / cellsize\n\n\n\nNow we specify the size of the raster we want with dimyx (note the order, y then x) using height and width.\nWe can convert this directly to a raster, but have to supply a CRS which we pull from the original points input dataset. At the time of writing (August 2021) you’ll get a complaint about the New Zealand Geodetic Datum 2000 because recent changes in how projections and datums are handled are still working themselves out.\n\n\nkde <- density(pts.ppp, sigma = 500, dimyx = c(height, width)) %>%\n  raster(crs = st_crs(pts)$wkt) # a ppp has no CRS information\n\n\n\nLet’s see what we got\nWe can map this - the default colour ramp in raster is horrible, so use RColorBrewer::brewer.pal to get something nicer.\n\n\ntm_shape(kde) +\n  tm_raster(palette = brewer.pal(9, \"Reds\"))\n\n\n\n\nA fallback sanity check\nTo give us an alternative view of the data, let’s just count points in polygons\n\n\npolys$n <- polys %>%\n  st_contains(pts) %>%\n  lengths()\n\n\n\nAnd map the result\n\n\ntm_shape(polys) +\n  tm_polygons(col = \"n\", palette = \"Reds\", title = \"Points in polygons\")\n\n\n\n\nAggregate the density surface pixels to polygons\nThis isn’t at all necessary, but is also useful to know. This is also a relatively slow operation. Note that we add together the density estimates in the pixels contained by each polygon.\n\n\nsummed_densities <- raster::extract(kde, polys, fun = sum)\n\n\n\nAppend this to the polygons and rescale so the result is an estimate of the original count. We multiply by cellsize^2 because each cell contains an estimate of the per sq metre (in this case, but per sq distance unit in general) density, so multiplying by the area of the cells gives an estimated count.\n\n\npolys$estimated_count = summed_densities[, 1] * cellsize ^ 2\n\n\n\nAnd now we can make another map\n\n\ntm_shape(polys) + \n  tm_polygons(col = \"estimated_count\", palette = \"Reds\", title = \"500m KDE estimate\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-10-21-kde/distill-preview.png",
    "last_modified": "2021-10-22T01:03:55+13:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-20-random-even-points-on-the-globe/",
    "title": "Uniform random points on the globe",
    "description": "There isn't as much land near the poles, so how do you make uniform\nrandomly distributed points in lat-lng coordinate space. Here's how!",
    "author": [],
    "date": "2021-10-20",
    "categories": [],
    "contents": "\nLibraries are the usual suspects plus rnaturalearth for basemap data\n\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(rnaturalearth)\nlibrary(dplyr)\n\nn <- 2500\n\n\n\nThe key thing to realise here is that random uniform numbers in both latitude and longitude will not be evenly distributed on Earth’s surface, because the meridians converge toward the poles. We can make two datasets to show this. First a naive set of randomly located points:\n\n\npts_naive <- data.frame(lon = runif(n) * 360 - 180,\n                        lat = runif(n) * 180 - 90,\n                        type = \"naive\")\n\n\n\nHere comes the science…\nAnd now a set where inserting a cosine correction ensures that the distribution of latitudes is appropriately more dense close to the equator:\n\n\npts_even <- data.frame(lon = runif(n) * 360 - 180,\n                       lat = acos(runif(n) * 2 - 1) * 180 / pi - 90,\n                       type = \"even\")\n\n\n\nCompare the latitude distributions\nWe can make up a combined data table and directly compare the distribution of the latitudes with a nice density plot. The increased representation of points in the mid-latitudes with the cosine correction is clear.\n\n\npts <- bind_rows(pts_naive, pts_even)\nggplot(pts) +\n  geom_density(aes(y = lat, fill = type), alpha = 0.5, lwd = 0) +\n  scale_fill_viridis_d()\n\n\n\n\nMake a map\nUse an equal-area projection to clearly see the problem geographically.\n\n\nw <- ne_countries(returnclass = \"sf\") %>%\n  st_transform(\"+proj=hammer\")\n\npts_sf <- pts %>%\n  st_as_sf(coords = 1:2, crs = 4326)\n\nggplot(w) + \n  geom_sf(fill = \"#cccccc\", colour = \"white\", lwd = 0.35) +\n  geom_sf(data = pts_sf, aes(colour = type), alpha = 0.35) +\n  scale_colour_viridis_d() +\n  theme_minimal()\n\n\n\n\nThe naively distributed points are clearly denser at the poles than they should be, where the cosine term in the ‘even’ points generation method makes them evenly distributed over the globe.\n\n\n\n",
    "preview": "posts/2021-10-20-random-even-points-on-the-globe/distill-preview.png",
    "last_modified": "2021-10-22T00:24:32+13:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
